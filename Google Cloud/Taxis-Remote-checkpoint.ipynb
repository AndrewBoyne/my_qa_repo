{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe5ec19-ca3c-4fdc-9c5c-81b5ef0fee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pickup_datetime     dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0  2018-03-27T13:17:01  2018-03-27T13:45:15                2           7.45   \n",
      "1  2018-08-18T22:48:08  2018-08-18T23:03:14                1           9.10   \n",
      "2  2018-01-07T15:03:56  2018-01-07T15:41:36                5          13.39   \n",
      "3  2018-08-29T16:46:15  2018-08-29T16:58:10                1           1.30   \n",
      "4  2018-11-07T15:41:10  2018-11-07T15:49:23                1           0.88   \n",
      "\n",
      "   payment_type  fare_amount  extra  tip_amount  total_amount dropoff_zone  \\\n",
      "0             1         25.5    0.0        0.00         26.30  Parkchester   \n",
      "1             1         25.5    0.5        6.50         39.06  Parkchester   \n",
      "2             1         41.5    0.0        9.61         57.67     Flatiron   \n",
      "3             2          9.0    1.0        0.00         10.80     Flatiron   \n",
      "4             2          6.5    0.0        0.00          7.30     Flatiron   \n",
      "\n",
      "  dropoff_borough  \n",
      "0           Bronx  \n",
      "1           Bronx  \n",
      "2       Manhattan  \n",
      "3       Manhattan  \n",
      "4       Manhattan  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "\n",
    "# Set the path to your credentials file\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"credentials.json\"\n",
    "project_id = 'lbg-labs-project2'\n",
    "bucket_name = 'lbg-labs-project2'\n",
    "file_name = 'ny-taxi-2018-sample.csv'\n",
    "dataset_id = 'trips'\n",
    "source_table_id = 'zone_id_mappings'\n",
    "destination_table_id = 'ny_taxi_with_zone_data'\n",
    "\n",
    "# Initialize Google Cloud Storage client\n",
    "gcsclient = storage.Client.from_service_account_json('credentials.json')\n",
    "\n",
    "# Get the GCS bucket\n",
    "bucket = gcsclient.get_bucket(bucket_name)\n",
    "\n",
    "# Specify the GCS blob for the source data\n",
    "blob = bucket.blob(file_name)\n",
    "\n",
    "# Initialize BigQuery client\n",
    "bqclient = bigquery.Client(project=project_id)\n",
    "\n",
    "# Download CSV data from GCS and read it into a Pandas DataFrame\n",
    "file_as_string = blob.download_as_string()\n",
    "taxi_data = pd.read_csv(StringIO(file_as_string.decode('utf-8')))\n",
    "\n",
    "# Filter out rows with trip_distance > 0\n",
    "taxi_data_filtered = taxi_data[taxi_data['trip_distance'] > 0.0]\n",
    "\n",
    "# Define the BigQuery table reference for the source table\n",
    "table_ref_source = bqclient.dataset(dataset_id).table(source_table_id)\n",
    "\n",
    "# Construct a SQL query to select all rows from the source table\n",
    "query = f\"SELECT * FROM `{table_ref_source.project}.{table_ref_source.dataset_id}.{table_ref_source.table_id}`\"\n",
    "\n",
    "# Execute the query and retrieve the result into a DataFrame\n",
    "query_job = bqclient.query(query)\n",
    "zone_data = query_job.to_dataframe()\n",
    "\n",
    "# Convert 'zone_id' columns to integer type for merging\n",
    "zone_data['zone_id'] = zone_data['zone_id'].astype(int)\n",
    "taxi_data_filtered = taxi_data_filtered.copy()\n",
    "taxi_data_filtered['dropoff_location_id'] = taxi_data_filtered['dropoff_location_id'].astype(int)\n",
    "\n",
    "# Merge the filtered taxi data with zone data based on 'dropoff_location_id'\n",
    "merged_data = taxi_data_filtered.merge(zone_data, how='inner', left_on='dropoff_location_id', right_on='zone_id')\n",
    "\n",
    "# Drop the specified columns\n",
    "merged_data = merged_data.drop(columns=['pickup_location_id', 'dropoff_location_id', 'zone_id'])\n",
    "\n",
    "# Rename the specified columns\n",
    "merged_data = merged_data.rename(columns={'zone_name': 'dropoff_zone', 'borough': 'dropoff_borough'})\n",
    "\n",
    "# Define the BigQuery destination dataset and table\n",
    "table_ref_sink = bqclient.dataset(dataset_id).table(destination_table_id)\n",
    "\n",
    "# Create a new table in BigQuery\n",
    "table = bigquery.Table(table_ref_sink)\n",
    "bqclient.create_table(table)\n",
    "\n",
    "# Configure the job for loading data into BigQuery\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    autodetect=True,\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # Overwrite existing data\n",
    ")\n",
    "\n",
    "# Load the merged data into the destination table in BigQuery\n",
    "bqclient.load_table_from_dataframe(merged_data, table_ref_sink, job_config=job_config).result()\n",
    "\n",
    "# Print the first few rows of the merged data\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14961635-6665-40da-a097-3078da89da4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02b46b-821e-42bd-9d73-86d4fc47df12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
